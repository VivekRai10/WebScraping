{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  WEB SCRAPPING ASSIGNMENT - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def Q1():\n",
    "    # Importing essential libraries\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import requests\n",
    "    import urllib.request\n",
    "    from urllib.request import urlopen\n",
    "    \n",
    "    \n",
    "    # Configure the webdriver\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(\"https://www.naukri.com/data-analyst-jobs-in-bangalore?k=data%20analyst&l=bangalore\")\n",
    "    \n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    \n",
    "    # Scrapping titles from the url\n",
    "    title = soup.find_all('div', class_ = 'info fleft')\n",
    "    Job_Title = []\n",
    "    for i in title:\n",
    "        Job_Title.append(i.a.text)\n",
    "    \n",
    "    \n",
    "    # Scrapping location from the url\n",
    "    location = soup.find_all('li', class_ = 'fleft grey-text br2 placeHolderLi location')\n",
    "    Job_Location = []\n",
    "    for i in location:\n",
    "        Job_Location.append(i.text)\n",
    "    \n",
    "    \n",
    "    # Scrapping company name from the url\n",
    "    coname = soup.find_all('div', class_ = 'mt-7 companyInfo subheading lh16')\n",
    "    Company_Name = []\n",
    "    for i in coname:\n",
    "        Company_Name.append(i.a.text)\n",
    "    \n",
    "    # Scrapping experience from the url\n",
    "    experience = soup.find_all('li', class_ = 'fleft grey-text br2 placeHolderLi experience')\n",
    "    Experience_Required = []\n",
    "    for i in experience:\n",
    "        Experience_Required.append(i.text)\n",
    "    \n",
    "    # Creating a dataframe object to store the scrapped data\n",
    "\n",
    "    DataAnalyst_Banglore = pd.DataFrame({'Job_Title' : Job_Title[0:10], 'Job_Location' : Job_Location[0:10], 'Company_Name' : Company_Name[0:10], 'Experience_Required' : Experience_Required[0:10] })\n",
    "    \n",
    "    # Returning the Dataframe\n",
    "    return DataAnalyst_Banglore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_Location</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Experience_Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Immediate opening For Data Scientist/Data Analyst</td>\n",
       "      <td>Chennai, Pune, Bengaluru, Hyderabad</td>\n",
       "      <td>CAIA-Center For Artificial Intelligence &amp; Adva...</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business Analyst and Data Analyst</td>\n",
       "      <td>Delhi NCR, Bengaluru, Hyderabad</td>\n",
       "      <td>Tech Mahindra Ltd.</td>\n",
       "      <td>7-12 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Chennai, Delhi NCR, Bengaluru</td>\n",
       "      <td>Hk solutions</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Intern - DFM Data Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>GLOBALFOUNDRIES Engineering Private Limited</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hiring Data Analysts on Contract Third party p...</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Flipkart Internet Private Limited</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reliability Data Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Alstom Transport India Ltd.</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Myntra</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Ladder of changes</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst / Business Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Altisource</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hiring For  Data Analyst RE) - Bangalore</td>\n",
       "      <td>Bengaluru(Bellandur)</td>\n",
       "      <td>TELEPERFORMANCE GLOBAL SERVICES</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job_Title  \\\n",
       "0  Immediate opening For Data Scientist/Data Analyst   \n",
       "1                  Business Analyst and Data Analyst   \n",
       "2                                       Data Analyst   \n",
       "3                          Intern - DFM Data Analyst   \n",
       "4  Hiring Data Analysts on Contract Third party p...   \n",
       "5                           Reliability Data Analyst   \n",
       "6                                Senior Data Analyst   \n",
       "7                                       Data Analyst   \n",
       "8                    Data Analyst / Business Analyst   \n",
       "9           Hiring For  Data Analyst RE) - Bangalore   \n",
       "\n",
       "                          Job_Location  \\\n",
       "0  Chennai, Pune, Bengaluru, Hyderabad   \n",
       "1      Delhi NCR, Bengaluru, Hyderabad   \n",
       "2        Chennai, Delhi NCR, Bengaluru   \n",
       "3                            Bengaluru   \n",
       "4                            Bengaluru   \n",
       "5                            Bengaluru   \n",
       "6                            Bengaluru   \n",
       "7                            Bengaluru   \n",
       "8                            Bengaluru   \n",
       "9                 Bengaluru(Bellandur)   \n",
       "\n",
       "                                        Company_Name Experience_Required  \n",
       "0  CAIA-Center For Artificial Intelligence & Adva...             0-3 Yrs  \n",
       "1                                 Tech Mahindra Ltd.            7-12 Yrs  \n",
       "2                                       Hk solutions             0-3 Yrs  \n",
       "3        GLOBALFOUNDRIES Engineering Private Limited             0-5 Yrs  \n",
       "4                  Flipkart Internet Private Limited             2-6 Yrs  \n",
       "5                        Alstom Transport India Ltd.             3-8 Yrs  \n",
       "6                                             Myntra             2-7 Yrs  \n",
       "7                                  Ladder of changes             0-5 Yrs  \n",
       "8                                         Altisource             1-6 Yrs  \n",
       "9                    TELEPERFORMANCE GLOBAL SERVICES             2-6 Yrs  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, full job-description. You have to scrape first 10 jobs data.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining function\n",
    "def Q2():\n",
    "    \n",
    "    # Importing essential libraries\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    \n",
    "    # Configuring the url\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(\"https://www.naukri.com/data-analyst-jobs-in-bangalore?k=data%20analyst&l=bangalore\")\n",
    "    \n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    # Scraping titles\n",
    "    title = soup.find_all('div', class_ = 'info fleft')\n",
    "    Job_Title = []\n",
    "    for i in title:\n",
    "        Job_Title.append(i.a.text)\n",
    "\n",
    "    \n",
    "    # Scaping locations\n",
    "    location = soup.find_all('li', class_ = 'fleft grey-text br2 placeHolderLi location')\n",
    "    Job_Location = []\n",
    "    for i in location:\n",
    "        Job_Location.append(i.text)\n",
    "    \n",
    "    \n",
    "    # Scraping company names\n",
    "    coname = soup.find_all('div', class_ = 'mt-7 companyInfo subheading lh16')\n",
    "    Company_Name = []\n",
    "    for i in coname:\n",
    "        Company_Name.append(i.a.text)\n",
    "        \n",
    "        \n",
    "     # Creating urls to scrape full descriptions   \n",
    "    desc_url1 = \"https://www.naukri.com/job-listings-immediate-opening-for-data-scientist-data-analyst-caia-center-for-artificial-intelligence-advanced-analytics-chennai-pune-bengaluru-bangalore-hyderabad-secunderabad-0-to-3-years-210920000599?src=jobsearchDesk&sid=16043158234975318&xp=1&px=1\"\n",
    "    desc_url2 = \"https://www.naukri.com/job-listings-senior-data-analyst-myntra-designs-pvt-ltd-bengaluru-bangalore-2-to-7-years-281020901652?src=jobsearchDesk&sid=16043158234975318&xp=2&px=1\"\n",
    "    desc_url3 = \"https://www.naukri.com/job-listings-senior-sourcing-data-analyst-coupa-erp-jaggaer-technicolor-bengaluru-bangalore-3-to-10-years-211020501122?src=jobsearchDesk&sid=16043158234975318&xp=3&px=1\"\n",
    "    desc_url4 = \"https://www.naukri.com/job-listings-data-analyst-ladder-of-changes-bengaluru-bangalore-0-to-5-years-311020900299?src=jobsearchDesk&sid=16043158234975318&xp=4&px=1\"\n",
    "    desc_url5 = \"https://www.naukri.com/job-listings-data-analyst-business-analyst-altisource-business-solutions-pvt-ltd-bengaluru-bangalore-1-to-6-years-291020005716?src=jobsearchDesk&sid=16043158234975318&xp=5&px=1\"\n",
    "    desc_url6 = \"https://www.naukri.com/job-listings-hiring-for-data-analyst-re-bangalore-teleperformance-global-services-pvt-ltd-bengaluru-bangalore-2-to-6-years-281020004246?src=jobsearchDesk&sid=16043158234975318&xp=6&px=1\"\n",
    "    desc_url7 = \"https://www.naukri.com/job-listings-data-analyst-pricing-zscaler-inc-pune-bengaluru-bangalore-chandigarh-3-to-6-years-191020501453?src=jobsearchDesk&sid=16043158234975318&xp=7&px=1\"\n",
    "    desc_url8 = \"https://www.naukri.com/job-listings-data-analyst-omega-seiki-chennai-pune-delhi-ncr-mumbai-ahmedabad-bengaluru-bangalore-surat-hyderabad-secunderabad-kolkata-0-to-5-years-311020002972?src=jobsearchDesk&sid=16043158234975318&xp=8&px=1\"\n",
    "    desc_url9 = \"https://www.naukri.com/job-listings-manager-data-analyst-brand-accelerator-myntra-designs-pvt-ltd-bengaluru-bangalore-2-to-7-years-231020900323?src=jobsearchDesk&sid=16043158234975318&xp=9&px=1\"\n",
    "    desc_url10 = \"https://www.naukri.com/job-listings-data-analyst-snaphunt-bengaluru-bangalore-3-to-5-years-030920000554?src=jobsearchDesk&sid=16043158234975318&xp=10&px=1\"\n",
    "    desc_url11 = \"https://www.naukri.com/job-listings-data-analyst-simplify360-bengaluru-bangalore-4-to-5-years-291020500625?src=jobsearchDesk&sid=16043158234975318&xp=11&px=1\"\n",
    "    desc_url12 = \"https://www.naukri.com/job-listings-data-analyst-and-dashboard-developer-qualitest-group-bengaluru-bangalore-2-to-5-years-221020501006?src=jobsearchDesk&sid=16043158234975318&xp=12&px=1\"\n",
    "    desc_url13 = \"https://www.naukri.com/job-listings-data-analyst-liventus-inc-bengaluru-bangalore-3-to-6-years-150720003694?src=jobsearchDesk&sid=16043158234975318&xp=13&px=1\"\n",
    "    desc_url14 = \"https://www.naukri.com/job-listings-business-analyst-data-analyst-dotsolved-india-pvt-ltd-chennai-delhi-ncr-bengaluru-bangalore-8-to-13-years-091020003800?src=jobsearchDesk&sid=16043158234975318&xp=14&px=1\"\n",
    "    desc_url15 = \"https://www.naukri.com/job-listings-business-data-analyst-kaplan-bengaluru-bangalore-2-to-5-years-301020501172?src=jobsearchDesk&sid=16043158234975318&xp=15&px=1\"\n",
    "    desc_url16 = \"https://www.naukri.com/job-listings-business-data-analyst-kaplan-test-prep-bengaluru-bangalore-2-to-5-years-301020500108?src=jobsearchDesk&sid=16043158234975318&xp=16&px=1\"\n",
    "    desc_url17 = \"https://www.naukri.com/job-listings-data-analyst-intertrustviteos-corporate-and-fund-services-pvt-ltd-chennai-mumbai-bengaluru-bangalore-7-to-10-years-301020004142?src=jobsearchDesk&sid=16043158234975318&xp=17&px=1\"\n",
    "    desc_url18 = \"https://www.naukri.com/job-listings-senior-data-analyst-python-sql-mysql-serving-skill-bengaluru-bangalore-6-to-9-years-111219901925?src=jobsearchDesk&sid=16043158234975318&xp=18&px=1\"\n",
    "    desc_url19 = \"https://www.naukri.com/job-listings-senior-clinical-data-analyst-navitas-life-science-chennai-bengaluru-bangalore-2-to-5-years-231020500044?src=jobsearchDesk&sid=16043158234975318&xp=19&px=1\"\n",
    "    desc_url20 = \"https://www.naukri.com/job-listings-clinical-data-analyst-navitas-life-science-chennai-bengaluru-bangalore-1-to-4-years-231020500042?src=jobsearchDesk&sid=16043158234975318&xp=20&px=1\"\n",
    "        \n",
    "    # Creating a dictionary to store the description    \n",
    "    desc = [desc_url1, desc_url2, desc_url3, desc_url4, desc_url5, desc_url6, desc_url7, desc_url8, desc_url9, desc_url10, desc_url11, desc_url12, desc_url13, desc_url14, desc_url15, desc_url16, desc_url17, desc_url18, desc_url19, desc_url20]\n",
    "    \n",
    "    \n",
    "    # Scraping descriptons\n",
    "    \n",
    "    driver.get(desc_url1)\n",
    "    content = driver.page_source\n",
    "    soup1 = BeautifulSoup(content)\n",
    "    job_desc = soup1.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description1 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description1.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url2)\n",
    "    content = driver.page_source\n",
    "    soup2 = BeautifulSoup(content)\n",
    "    job_desc = soup2.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description2 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description2.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url3)\n",
    "    content = driver.page_source\n",
    "    soup3 = BeautifulSoup(content)\n",
    "    job_desc = soup3.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description3 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description3.append(i.text)\n",
    "\n",
    "    driver.get(desc_url4)\n",
    "    content = driver.page_source\n",
    "    soup4 = BeautifulSoup(content)\n",
    "    job_desc = soup4.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description4 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description4.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url5)\n",
    "    content = driver.page_source\n",
    "    soup5 = BeautifulSoup(content)\n",
    "    job_desc = soup5.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description5 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description5.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url6)\n",
    "    content = driver.page_source\n",
    "    soup6 = BeautifulSoup(content)\n",
    "    job_desc = soup6.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description6 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description6.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url7)\n",
    "    content = driver.page_source\n",
    "    soup7 = BeautifulSoup(content)\n",
    "    job_desc = soup7.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description7 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description7.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url8)\n",
    "    content = driver.page_source\n",
    "    soup8 = BeautifulSoup(content)\n",
    "    job_desc = soup8.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description8 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description8.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url9)\n",
    "    content = driver.page_source\n",
    "    soup9 = BeautifulSoup(content)\n",
    "    job_desc = soup9.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description9 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description9.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url10)\n",
    "    content = driver.page_source\n",
    "    soup10 = BeautifulSoup(content)\n",
    "    job_desc = soup10.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description10 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description10.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url11)\n",
    "    content = driver.page_source\n",
    "    soup11 = BeautifulSoup(content)\n",
    "    job_desc = soup11.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description11 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description11.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url12)\n",
    "    content = driver.page_source\n",
    "    soup12 = BeautifulSoup(content)\n",
    "    job_desc = soup12.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description12 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description12.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url13)\n",
    "    content = driver.page_source\n",
    "    soup13 = BeautifulSoup(content)\n",
    "    job_desc = soup13.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description13 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description13.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url14)\n",
    "    content = driver.page_source\n",
    "    soup14 = BeautifulSoup(content)\n",
    "    job_desc = soup14.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description14 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description14.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url15)\n",
    "    content = driver.page_source\n",
    "    soup15 = BeautifulSoup(content)\n",
    "    job_desc = soup15.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description15 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description15.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url16)\n",
    "    content = driver.page_source\n",
    "    soup16 = BeautifulSoup(content)\n",
    "    job_desc = soup16.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description16 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description16.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url17)\n",
    "    content = driver.page_source\n",
    "    soup17 = BeautifulSoup(content)\n",
    "    job_desc = soup17.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description17 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description17.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url18)\n",
    "    content = driver.page_source\n",
    "    soup18 = BeautifulSoup(content)\n",
    "    job_desc = soup18.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description18 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description18.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url19)\n",
    "    content = driver.page_source\n",
    "    soup19 = BeautifulSoup(content)\n",
    "    job_desc = soup19.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description19 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description19.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url20)\n",
    "    content = driver.page_source\n",
    "    soup20 = BeautifulSoup(content)\n",
    "    job_desc = soup20.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description20 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description20.append(i.text)\n",
    "        \n",
    "    # Appending all the scrapped descriptions\n",
    "    JD = Job_Description1+Job_Description2+Job_Description3+Job_Description4+Job_Description5+Job_Description6+Job_Description7+Job_Description8+Job_Description9+Job_Description10+Job_Description11+Job_Description12+Job_Description13+Job_Description14+Job_Description15+Job_Description16+Job_Description17+Job_Description18+Job_Description19+Job_Description20\n",
    "    \n",
    "    \n",
    "    # Creating a dataframe to store the scrapped information\n",
    "    Naukri_listng = pd.DataFrame({'Job_Title' : Job_Title[0:10], 'Job_Location' : Job_Location[0:10], 'Company_Name' : Company_Name[0:10], 'Job_Description' : JD[0:10]})\n",
    "    \n",
    "    # Returning the dataframe\n",
    "    return Naukri_listng\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_Location</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Job_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Immediate opening For Data Scientist/Data Analyst</td>\n",
       "      <td>Chennai, Pune, Bengaluru, Hyderabad</td>\n",
       "      <td>CAIA-Center For Artificial Intelligence &amp; Adva...</td>\n",
       "      <td>Job descriptionRoles and ResponsibilitiesSENIO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business Analyst and Data Analyst</td>\n",
       "      <td>Delhi NCR, Bengaluru, Hyderabad</td>\n",
       "      <td>Tech Mahindra Ltd.</td>\n",
       "      <td>Job description     Key roles of this position...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Chennai, Delhi NCR, Bengaluru</td>\n",
       "      <td>Hk solutions</td>\n",
       "      <td>Job descriptionRoles and ResponsibilitiesRespo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Intern - DFM Data Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>GLOBALFOUNDRIES Engineering Private Limited</td>\n",
       "      <td>Job descriptionGreetings from “Altisource busi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hiring Data Analysts on Contract Third party p...</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Flipkart Internet Private Limited</td>\n",
       "      <td>Job descriptionHiring for Data Analyst(RE)Qual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reliability Data Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Alstom Transport India Ltd.</td>\n",
       "      <td>Job description We?re looking for a data analy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Myntra</td>\n",
       "      <td>Job descriptionRoles and ResponsibilitiesRespo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Ladder of changes</td>\n",
       "      <td>Job description The Offer Join an exciting, aw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst / Business Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Altisource</td>\n",
       "      <td>Job description Responsibilities:   ? Managing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hiring For  Data Analyst RE) - Bangalore</td>\n",
       "      <td>Bengaluru(Bellandur)</td>\n",
       "      <td>TELEPERFORMANCE GLOBAL SERVICES</td>\n",
       "      <td>Job description  Job description     Any Datab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job_Title  \\\n",
       "0  Immediate opening For Data Scientist/Data Analyst   \n",
       "1                  Business Analyst and Data Analyst   \n",
       "2                                       Data Analyst   \n",
       "3                          Intern - DFM Data Analyst   \n",
       "4  Hiring Data Analysts on Contract Third party p...   \n",
       "5                           Reliability Data Analyst   \n",
       "6                                Senior Data Analyst   \n",
       "7                                       Data Analyst   \n",
       "8                    Data Analyst / Business Analyst   \n",
       "9           Hiring For  Data Analyst RE) - Bangalore   \n",
       "\n",
       "                          Job_Location  \\\n",
       "0  Chennai, Pune, Bengaluru, Hyderabad   \n",
       "1      Delhi NCR, Bengaluru, Hyderabad   \n",
       "2        Chennai, Delhi NCR, Bengaluru   \n",
       "3                            Bengaluru   \n",
       "4                            Bengaluru   \n",
       "5                            Bengaluru   \n",
       "6                            Bengaluru   \n",
       "7                            Bengaluru   \n",
       "8                            Bengaluru   \n",
       "9                 Bengaluru(Bellandur)   \n",
       "\n",
       "                                        Company_Name  \\\n",
       "0  CAIA-Center For Artificial Intelligence & Adva...   \n",
       "1                                 Tech Mahindra Ltd.   \n",
       "2                                       Hk solutions   \n",
       "3        GLOBALFOUNDRIES Engineering Private Limited   \n",
       "4                  Flipkart Internet Private Limited   \n",
       "5                        Alstom Transport India Ltd.   \n",
       "6                                             Myntra   \n",
       "7                                  Ladder of changes   \n",
       "8                                         Altisource   \n",
       "9                    TELEPERFORMANCE GLOBAL SERVICES   \n",
       "\n",
       "                                     Job_Description  \n",
       "0  Job descriptionRoles and ResponsibilitiesSENIO...  \n",
       "1  Job description     Key roles of this position...  \n",
       "2  Job descriptionRoles and ResponsibilitiesRespo...  \n",
       "3  Job descriptionGreetings from “Altisource busi...  \n",
       "4  Job descriptionHiring for Data Analyst(RE)Qual...  \n",
       "5  Job description We?re looking for a data analy...  \n",
       "6  Job descriptionRoles and ResponsibilitiesRespo...  \n",
       "7  Job description The Offer Join an exciting, aw...  \n",
       "8  Job description Responsibilities:   ? Managing...  \n",
       "9  Job description  Job description     Any Datab...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "***Q3: In this question you have to scrape data using the filters available on the webpage:***\n",
    "\n",
    "You have to use the location and salary filter.\n",
    "\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "\n",
    "You have to scrape the job-title, job-location, company_name,\n",
    "\n",
    "experience_required.\n",
    "\n",
    "The location filter to be used is “Delhi/NCR”\n",
    "\n",
    "The salary filter to be used is “3-6” lakhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q3():\n",
    "    # Importing essential libraries\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import requests\n",
    "    import urllib.request\n",
    "    from urllib.request import urlopen\n",
    "    \n",
    "    \n",
    "    # Configure the webdriver\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(\"https://www.naukri.com/data-scientist-jobs?k=data%20scientist&ctcFilter=3to6&cityType=25.9.31\")\n",
    "    \n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    \n",
    "    # Scrapping titles from the url\n",
    "    title = soup.find_all('div', class_ = 'info fleft')\n",
    "    Job_Title = []\n",
    "    for i in title:\n",
    "        Job_Title.append(i.a.text)\n",
    "    \n",
    "    \n",
    "    # Scrapping location from the url\n",
    "    location = soup.find_all('li', class_ = 'fleft grey-text br2 placeHolderLi location')\n",
    "    Job_Location = []\n",
    "    for i in location:\n",
    "        Job_Location.append(i.text)\n",
    "    \n",
    "    \n",
    "    # Scrapping company name from the url\n",
    "    coname = soup.find_all('div', class_ = 'mt-7 companyInfo subheading lh16')\n",
    "    Company_Name = []\n",
    "    for i in coname:\n",
    "        Company_Name.append(i.a.text)\n",
    "    \n",
    "    # Scrapping experience from the url\n",
    "    experience = soup.find_all('li', class_ = 'fleft grey-text br2 placeHolderLi experience')\n",
    "    Experience_Required = []\n",
    "    for i in experience:\n",
    "        Experience_Required.append(i.text)\n",
    "    \n",
    "    # Creating a dataframe object to store the scrapped data\n",
    "\n",
    "    Dat_ascientist_DelhiNCR = pd.DataFrame({'Job_Title' : Job_Title[0:10], 'Job_Location' : Job_Location[0:10], 'Company_Name' : Company_Name[0:10], 'Experience_Required' : Experience_Required[0:10] })\n",
    "    \n",
    "    # Returning the Dataframe\n",
    "    return Dat_ascientist_DelhiNCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_Location</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Experience_Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Noida</td>\n",
       "      <td>AlgoScale Technologies Private Limited</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist/Analyst - Machine Learning/Deep...</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>EchoIndia</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist - NLP/ML/Python</td>\n",
       "      <td>Gurgaon Gurugram</td>\n",
       "      <td>Elixir Web Solutions</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist - NLP/ML/Python</td>\n",
       "      <td>Gurgaon Gurugram</td>\n",
       "      <td>Elixir Web Solutions</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>GlobalHunt India Private Limited</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Blue Sky Analytics</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist/BPM/4-8 years</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Crescendo global services</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst/Scientist</td>\n",
       "      <td>Delhi NCR, Ghaziabad</td>\n",
       "      <td>Amity University</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist Machine Learning</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Delhivery</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Business Analyst - Data Scientist</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>HyreFox Consultants Pvt Ltd</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job_Title          Job_Location  \\\n",
       "0                                     Data Scientist                 Noida   \n",
       "1  Data Scientist/Analyst - Machine Learning/Deep...                 Delhi   \n",
       "2                    Data Scientist - NLP/ML/Python       Gurgaon Gurugram   \n",
       "3                    Data Scientist - NLP/ML/Python       Gurgaon Gurugram   \n",
       "4                                     Data Scientist                 Delhi   \n",
       "5                                     Data Scientist               Gurgaon   \n",
       "6                       Data Scientist/BPM/4-8 years               Gurgaon   \n",
       "7                             Data Analyst/Scientist  Delhi NCR, Ghaziabad   \n",
       "8                    Data Scientist Machine Learning               Gurgaon   \n",
       "9                  Business Analyst - Data Scientist               Gurgaon   \n",
       "\n",
       "                             Company_Name Experience_Required  \n",
       "0  AlgoScale Technologies Private Limited             1-5 Yrs  \n",
       "1                               EchoIndia             3-6 Yrs  \n",
       "2                    Elixir Web Solutions             4-8 Yrs  \n",
       "3                    Elixir Web Solutions             4-8 Yrs  \n",
       "4       GlobalHunt India Private Limited              3-6 Yrs  \n",
       "5                      Blue Sky Analytics             1-6 Yrs  \n",
       "6               Crescendo global services             4-7 Yrs  \n",
       "7                        Amity University             6-8 Yrs  \n",
       "8                               Delhivery             1-3 Yrs  \n",
       "9             HyreFox Consultants Pvt Ltd             3-5 Yrs  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "***Q4: Write a python program to scrape data for first 10 job results for Data scientist Designation in Noida location. You have to scrape company_name, No. of days ago when job was posted, Rating of the company.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q4():\n",
    "    \n",
    "\n",
    "# Importing essential libraries\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import requests\n",
    "    import urllib.request\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "    \n",
    "# Configure the webdriver\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(\"https://www.glassdoor.co.in/Job/noida-data-scientist-jobs-SRCH_IL.0,5_IC4477468_KO6,20.htm\")\n",
    "    \n",
    "# Creating soup\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    co_name = soup.find_all('div', class_ = 'jobHeader d-flex justify-content-between align-items-start')\n",
    "\n",
    "    Company_Name = []\n",
    "\n",
    "    for i in co_name:\n",
    "        Company_Name.append(i.text)\n",
    "        \n",
    "        \n",
    "    job_posted = soup.find_all('div', class_ = 'd-flex align-items-end pl-std css-mi55ob')\n",
    "\n",
    "    Posted_Days_Ago = []\n",
    "\n",
    "    for i in job_posted:\n",
    "        Posted_Days_Ago.append(i.text)\n",
    "        \n",
    "        \n",
    "    rating = soup.find_all('span', class_ = 'compactStars')\n",
    "\n",
    "    Rating = []\n",
    "\n",
    "    for i in rating:\n",
    "        Rating.append(i.text)\n",
    "        \n",
    "        \n",
    "        \n",
    "# Creating a dataframe object to store the scrapped data\n",
    "\n",
    "    Glassdor_Noida = pd.DataFrame({'Company_Name' : Company_Name[0:10], 'Posted_Days_Ago' : Posted_Days_Ago[0:10], 'Rating' : Rating[0:10]})\n",
    "    \n",
    "# Returning the Dataframe\n",
    "    return Glassdor_Noida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Posted_Days_Ago</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xtLytics</td>\n",
       "      <td>3d</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RiseIn Technologies Pvt Ltd</td>\n",
       "      <td>30d+</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jubna</td>\n",
       "      <td>7d</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EzappSolution</td>\n",
       "      <td>30d</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xtLytics</td>\n",
       "      <td>3d</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>McKinsey &amp; Company</td>\n",
       "      <td>2d</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ericsson</td>\n",
       "      <td>3d</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Biz2Credit Inc</td>\n",
       "      <td>26d</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>UnitedHealth Group</td>\n",
       "      <td>4d</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Salasar New Age Technologies</td>\n",
       "      <td>30d+</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Company_Name Posted_Days_Ago Rating\n",
       "0                      xtLytics              3d      3\n",
       "1   RiseIn Technologies Pvt Ltd            30d+    3.1\n",
       "2                         Jubna              7d      5\n",
       "3                 EzappSolution             30d      3\n",
       "4                      xtLytics              3d    4.5\n",
       "5            McKinsey & Company              2d    4.2\n",
       "6                      Ericsson              3d    3.7\n",
       "7                Biz2Credit Inc             26d    3.4\n",
       "8            UnitedHealth Group              4d    3.9\n",
       "9  Salasar New Age Technologies            30d+      5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "***Q5: Write a python program to scrape the salary data for Data Scientist designation in Noida location.You have to scrape Company name, Number of salaries, Average salary, Min salary, Max Salary.***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q5():\n",
    "    \n",
    "    # Importing essential libraries\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import requests\n",
    "    import urllib.request\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "\n",
    "    # Configure the webdriver\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(\"https://www.glassdoor.co.in/Salaries/new-delhi-data-scientist-salary-SRCH_IL.0,9_IM1083_KO10,24.htm\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    \n",
    "    #min_salary = soup.find_all('div', class_ = 'common__RangeBarStyle__values common__flex__justifySpaceBetween common__flex__container')\n",
    "    #Min_Salary = []\n",
    "    #for i in min_salary:\n",
    "    #    Min_Salary.append(i.span.text)\n",
    "    #len(Min_Salary)\n",
    "    \n",
    "    # scrapping salaries\n",
    "    min_max_salary = soup.find_all('div', class_ = 'common__RangeBarStyle__values common__flex__justifySpaceBetween common__flex__container')\n",
    "    Min_Max_Salary = []\n",
    "    for i in min_max_salary:\n",
    "        Min_Max_Salary.append(i.text)\n",
    "        \n",
    "    # Scrapping average salary\n",
    "    avg_sal = soup.find_all('div', class_ = 'col-2 d-none d-md-flex flex-row justify-content-end')\n",
    "    Average_Salary = []\n",
    "    for i in avg_sal:\n",
    "        Average_Salary.append(i.strong.text)\n",
    "        \n",
    "    # Scrapping no. of salaries    \n",
    "    no_sal = soup.find_all('p', class_ = 'css-1uyte9r css-1kuy7z7 m-0')\n",
    "    No_of_Salaries = []\n",
    "    for i in no_sal:\n",
    "        No_of_Salaries.append(i.text)\n",
    "        \n",
    "    # Scrapping the name of the company    \n",
    "    name = soup.find_all('p', class_ = 'm-0')\n",
    "    access_dict = [2,8,14,20,26,32,38,44,50,56,62,68,74,80,86,92,98,104,110,116]\n",
    "    Company_name = []\n",
    "    for i in access_dict:\n",
    "        Company_name.append(name[i].text)\n",
    "        \n",
    "    # Returning the dataframe    \n",
    "    Glassdoor_Noida = pd.DataFrame({'Company_name' : Company_name[0:10], 'No_of_Salaries' : No_of_Salaries[0:10], 'Average_Salary' : Average_Salary[0:10], 'Min_Max_Salary' : Min_Max_Salary[0:10]})\n",
    "    return Glassdoor_Noida\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company_name</th>\n",
       "      <th>No_of_Salaries</th>\n",
       "      <th>Average_Salary</th>\n",
       "      <th>Min_Max_Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Delhivery</td>\n",
       "      <td>11 salaries</td>\n",
       "      <td>₹ 13,18,563</td>\n",
       "      <td>₹706K₹11,513K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accenture</td>\n",
       "      <td>8 salaries</td>\n",
       "      <td>₹ 9,85,497</td>\n",
       "      <td>₹572K₹1,300K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IBM</td>\n",
       "      <td>7 salaries</td>\n",
       "      <td>₹ 7,53,602</td>\n",
       "      <td>₹581K₹2,704K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UnitedHealth Group</td>\n",
       "      <td>7 salaries</td>\n",
       "      <td>₹ 13,23,634</td>\n",
       "      <td>₹710K₹1,559K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cognizant Technology Solutions</td>\n",
       "      <td>6 salaries</td>\n",
       "      <td>₹ 9,97,979</td>\n",
       "      <td>₹785K₹1,251K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Valiance Solutions</td>\n",
       "      <td>6 salaries</td>\n",
       "      <td>₹ 7,72,507</td>\n",
       "      <td>₹497K₹1,140K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Vidooly Media Tech</td>\n",
       "      <td>6 salaries</td>\n",
       "      <td>₹ 12,689</td>\n",
       "      <td>₹8K₹20K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Analytics Vidhya</td>\n",
       "      <td>6 salaries</td>\n",
       "      <td>₹ 21,215</td>\n",
       "      <td>₹14K₹22K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tata Consultancy Services</td>\n",
       "      <td>5 salaries</td>\n",
       "      <td>₹ 6,77,498</td>\n",
       "      <td>₹480K₹1,000K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ericsson-Worldwide</td>\n",
       "      <td>5 salaries</td>\n",
       "      <td>₹ 7,34,456</td>\n",
       "      <td>₹460K₹1,598K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Company_name No_of_Salaries Average_Salary Min_Max_Salary\n",
       "0                       Delhivery    11 salaries    ₹ 13,18,563  ₹706K₹11,513K\n",
       "1                       Accenture     8 salaries     ₹ 9,85,497   ₹572K₹1,300K\n",
       "2                             IBM     7 salaries     ₹ 7,53,602   ₹581K₹2,704K\n",
       "3              UnitedHealth Group     7 salaries    ₹ 13,23,634   ₹710K₹1,559K\n",
       "4  Cognizant Technology Solutions     6 salaries     ₹ 9,97,979   ₹785K₹1,251K\n",
       "5              Valiance Solutions     6 salaries     ₹ 7,72,507   ₹497K₹1,140K\n",
       "6              Vidooly Media Tech     6 salaries       ₹ 12,689        ₹8K₹20K\n",
       "7                Analytics Vidhya     6 salaries       ₹ 21,215       ₹14K₹22K\n",
       "8       Tata Consultancy Services     5 salaries     ₹ 6,77,498   ₹480K₹1,000K\n",
       "9              Ericsson-Worldwide     5 salaries     ₹ 7,34,456   ₹460K₹1,598K"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective 6 : \n",
    "\n",
    "***Q6: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:***\n",
    "\n",
    "1. Brand\n",
    "\n",
    "2. Product Description\n",
    "\n",
    "3. Price\n",
    "\n",
    "4. Discount %\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q6():\n",
    "    \n",
    "    # Importing essential libraries\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import requests\n",
    "    import urllib.request\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "\n",
    "    # Configure the webdriver\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(\"https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    brand = soup.find_all('div', class_ = '_2B_pmu')\n",
    "    Brand = []\n",
    "    for i in brand:\n",
    "        Brand.append(i.text[0:40])\n",
    "\n",
    "    # Scraping description of the product\n",
    "    description = soup.find_all('a', class_ = '_2mylT6')\n",
    "    Product_Description = []\n",
    "    for i in description:\n",
    "        Product_Description.append(i.text)\n",
    "\n",
    "    # Scraping prce of the product\n",
    "    price = soup.find_all('div', class_ = '_1vC4OE')\n",
    "    Price = []\n",
    "    for i in price:\n",
    "        Price.append(i.text[0:40])\n",
    "\n",
    "    # Scraping discount on the product\n",
    "    discount = soup.find_all('div', class_ = 'VGWI6T')\n",
    "    Discount = []\n",
    "    for i in discount:\n",
    "        Discount.append(i)\n",
    "\n",
    "    \n",
    "    # Scaping next page for more results\n",
    "    driver.get(\"https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=2\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup1 = BeautifulSoup(content)\n",
    "    \n",
    "    # Scraping brand of the products - page 2\n",
    "    brand1 = soup1.find_all('div', class_ = '_2B_pmu')\n",
    "    Brand1 = []\n",
    "    for i in brand1:\n",
    "        Brand1.append(i.text[0:40])\n",
    "\n",
    "    # Sctraping description of the products - page 2\n",
    "    description1 = soup1.find_all('a', class_ = '_2mylT6')\n",
    "    Product_Description1 = []\n",
    "    for i in description1:\n",
    "        Product_Description1.append(i.text)\n",
    "\n",
    "    # Scraping price of the products - page 2\n",
    "    price1 = soup1.find_all('div', class_ = '_1vC4OE')\n",
    "    Price1 = []\n",
    "    for i in price1:\n",
    "        Price1.append(i.text[0:40])\n",
    "\n",
    "    # Scraping discount on the products - page 2\n",
    "    discount1 = soup1.find_all('div', class_ = 'VGWI6T')\n",
    "    Discount1 = []\n",
    "    for i in discount1:\n",
    "        Discount1.append(i)\n",
    "\n",
    "    # Scraping next page for more results\n",
    "    driver.get(\"https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=3\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup2 = BeautifulSoup(content)\n",
    "    \n",
    "    # Scraping brand of the product - page 3\n",
    "    brand2 = soup2.find_all('div', class_ = '_2B_pmu')\n",
    "    Brand2 = []\n",
    "    for i in brand2:\n",
    "        Brand2.append(i.text[0:40])\n",
    "\n",
    "    # Scraping description of the product - page 3\n",
    "    description2 = soup2.find_all('a', class_ = '_2mylT6')\n",
    "    Product_Description2 = []\n",
    "    for i in description2:\n",
    "        Product_Description2.append(i.text)\n",
    "\n",
    "    # Scraping price of the product - page 3\n",
    "    price2 = soup2.find_all('div', class_ = '_1vC4OE')\n",
    "    Price2 = []\n",
    "    for i in price2:\n",
    "        Price2.append(i.text[0:40])\n",
    "    \n",
    "    # Scraping discounts on the products - page 3\n",
    "    discount2 = soup2.find_all('div', class_ = 'VGWI6T')\n",
    "    Discount2 = []\n",
    "    for i in discount2:\n",
    "        Discount2.append(i)\n",
    "\n",
    "    # Appending lists of all the pages and scrutinizing it appropriately.\n",
    "    BRAND = Brand2+Brand1+Brand2\n",
    "    PRODUCT_DESCRIPTION = Product_Description+Product_Description1+Product_Description2\n",
    "    PRICE = Price[0:40]+Price1[0:40]+Price2[0:40]\n",
    "    DISCOUNT = Discount[0:40]+Discount1[0:40]+Discount2[0:40]\n",
    "    \n",
    "    # Creating a dataframe to store the scrapped information\n",
    "    Sunglasses = pd.DataFrame({'BRAND' : BRAND[0:100], 'PRODUCT_DESCRIPTION' : PRODUCT_DESCRIPTION[0:100], 'PRICE' : PRICE[0:100], 'DISCOUNT' : DISCOUNT[0:100]})\n",
    "\n",
    "    # Returning the dataframe\n",
    "    return Sunglasses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BRAND</th>\n",
       "      <th>PRODUCT_DESCRIPTION</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>DISCOUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INSH</td>\n",
       "      <td>UV Protection Round Sunglasses (51)</td>\n",
       "      <td>₹2,640</td>\n",
       "      <td>[[40% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JUST STYLE</td>\n",
       "      <td>Others Rectangular Sunglasses (54)</td>\n",
       "      <td>₹2,200</td>\n",
       "      <td>[[50% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Billion</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹674</td>\n",
       "      <td>[[25% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fossil</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>₹312</td>\n",
       "      <td>[[87% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IRUS by IDEE</td>\n",
       "      <td>Gradient, Mirrored, UV Protection Aviator Sung...</td>\n",
       "      <td>₹199</td>\n",
       "      <td>[[86% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>INSH</td>\n",
       "      <td>UV Protection, Night Vision, Riding Glasses Re...</td>\n",
       "      <td>₹327</td>\n",
       "      <td>[[86% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Eyevy</td>\n",
       "      <td>Gradient, UV Protection, Others Retro Square S...</td>\n",
       "      <td>₹379</td>\n",
       "      <td>[[62% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>New Specs</td>\n",
       "      <td>Mirrored, UV Protection, Riding Glasses, Other...</td>\n",
       "      <td>₹296</td>\n",
       "      <td>[[78% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Trendy Glasses</td>\n",
       "      <td>UV Protection Retro Square Sunglasses (Free Size)</td>\n",
       "      <td>₹227</td>\n",
       "      <td>[[88% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Fossil</td>\n",
       "      <td>Gradient Rectangular Sunglasses (53)</td>\n",
       "      <td>₹2,420</td>\n",
       "      <td>[[45% off]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             BRAND                                PRODUCT_DESCRIPTION   PRICE  \\\n",
       "0             INSH                UV Protection Round Sunglasses (51)  ₹2,640   \n",
       "1       JUST STYLE                 Others Rectangular Sunglasses (54)  ₹2,200   \n",
       "2          Billion      UV Protection Wayfarer Sunglasses (Free Size)    ₹674   \n",
       "3           Fossil                UV Protection Round Sunglasses (54)    ₹312   \n",
       "4     IRUS by IDEE  Gradient, Mirrored, UV Protection Aviator Sung...    ₹199   \n",
       "..             ...                                                ...     ...   \n",
       "95            INSH  UV Protection, Night Vision, Riding Glasses Re...    ₹327   \n",
       "96           Eyevy  Gradient, UV Protection, Others Retro Square S...    ₹379   \n",
       "97       New Specs  Mirrored, UV Protection, Riding Glasses, Other...    ₹296   \n",
       "98  Trendy Glasses  UV Protection Retro Square Sunglasses (Free Size)    ₹227   \n",
       "99          Fossil               Gradient Rectangular Sunglasses (53)  ₹2,420   \n",
       "\n",
       "       DISCOUNT  \n",
       "0   [[40% off]]  \n",
       "1   [[50% off]]  \n",
       "2   [[25% off]]  \n",
       "3   [[87% off]]  \n",
       "4   [[86% off]]  \n",
       "..          ...  \n",
       "95  [[86% off]]  \n",
       "96  [[62% off]]  \n",
       "97  [[78% off]]  \n",
       "98  [[88% off]]  \n",
       "99  [[45% off]]  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "***Q7: Scrape 100 reviews data from flipkart.com for iphone11 phone.***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q7():\n",
    "\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import requests\n",
    "    import urllib.request\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "\n",
    "    # Configure the webdriver\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    \n",
    "    rating = soup.find_all('div', class_ = 'hGSR34 E_uFuv')\n",
    "\n",
    "    Ratings = []\n",
    "\n",
    "    for i in rating:\n",
    "        Ratings.append(i.text)\n",
    "        \n",
    "        \n",
    "    rev_sum = soup.find_all('p', class_ = '_2xg6Ul')\n",
    "    Rev_Sum = []\n",
    "    for i in rev_sum:\n",
    "        Rev_Sum.append(i.text)\n",
    "        \n",
    "    rev = soup.find_all('div', class_ = 'qwjRop')\n",
    "    Review = []\n",
    "    for i in rev:\n",
    "        Review.append(i.text)\n",
    "        \n",
    "        \n",
    "    driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=2\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup2 = BeautifulSoup(content)\n",
    "\n",
    "\n",
    "    driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=3\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup3 = BeautifulSoup(content)\n",
    "        \n",
    "\n",
    "    driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=4\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup4 = BeautifulSoup(content)\n",
    "    \n",
    "    \n",
    "    driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=5\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup5 = BeautifulSoup(content)\n",
    "    \n",
    "    \n",
    "    driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=6\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup6 = BeautifulSoup(content)\n",
    "    \n",
    "    \n",
    "    driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=7\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup7 = BeautifulSoup(content)\n",
    "    \n",
    "    \n",
    "    driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=8\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup8 = BeautifulSoup(content)\n",
    "    \n",
    "    \n",
    "    driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=9\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup9 = BeautifulSoup(content)\n",
    "    \n",
    "    \n",
    "    driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=10\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup10 = BeautifulSoup(content)\n",
    "    \n",
    "    \n",
    "    rating2 = soup2.find_all('div', class_ = 'hGSR34 E_uFuv')\n",
    "    Ratings2 = []\n",
    "    for i in rating2:\n",
    "        Ratings2.append(i.text)\n",
    "    Ratings2.insert(-1, 1)\n",
    "\n",
    "    rev_sum2 = soup2.find_all('p', class_ = '_2xg6Ul')\n",
    "    Rev_Sum2 = []\n",
    "    for i in rev_sum2:\n",
    "        Rev_Sum2.append(i.text)\n",
    "\n",
    "    rev2 = soup2.find_all('div', class_ = 'qwjRop')\n",
    "    Review2 = []\n",
    "    for i in rev2:\n",
    "        Review2.append(i.text)\n",
    "\n",
    "        \n",
    "    rating3 = soup3.find_all('div', class_ = 'hGSR34 E_uFuv')\n",
    "    Ratings3 = []\n",
    "    for i in rating3:\n",
    "        Ratings3.append(i.text)\n",
    "\n",
    "    rev_sum3 = soup3.find_all('p', class_ = '_2xg6Ul')\n",
    "    Rev_Sum3 = []\n",
    "    for i in rev_sum3:\n",
    "        Rev_Sum3.append(i.text)\n",
    "\n",
    "    rev3 = soup3.find_all('div', class_ = 'qwjRop')\n",
    "    Review3 = []\n",
    "    for i in rev3:\n",
    "        Review3.append(i.text)\n",
    "        \n",
    "        \n",
    "        \n",
    "    rating4 = soup4.find_all('div', class_ = 'hGSR34 E_uFuv')\n",
    "    Ratings4 = []\n",
    "    for i in rating4:\n",
    "        Ratings4.append(i.text)\n",
    "\n",
    "    rev_sum4 = soup4.find_all('p', class_ = '_2xg6Ul')\n",
    "    Rev_Sum4 = []\n",
    "    for i in rev_sum4:\n",
    "        Rev_Sum4.append(i.text)\n",
    "\n",
    "    rev4 = soup4.find_all('div', class_ = 'qwjRop')\n",
    "    Review4 = []\n",
    "    for i in rev4:\n",
    "        Review4.append(i.text) \n",
    "        \n",
    "        \n",
    "        \n",
    "    rating5 = soup5.find_all('div', class_ = 'hGSR34 E_uFuv')\n",
    "    Ratings5 = []\n",
    "    for i in rating5:\n",
    "        Ratings5.append(i.text)\n",
    "\n",
    "    rev_sum5 = soup5.find_all('p', class_ = '_2xg6Ul')\n",
    "    Rev_Sum5 = []\n",
    "    for i in rev_sum5:\n",
    "        Rev_Sum5.append(i.text)\n",
    "\n",
    "    rev5 = soup5.find_all('div', class_ = 'qwjRop')\n",
    "    Review5 = []\n",
    "    for i in rev5:\n",
    "        Review5.append(i.text)\n",
    "        \n",
    "        \n",
    "    rating6 = soup6.find_all('div', class_ = 'hGSR34 E_uFuv')\n",
    "    Ratings6 = []\n",
    "    for i in rating6:\n",
    "        Ratings6.append(i.text)\n",
    "\n",
    "    rev_sum6 = soup6.find_all('p', class_ = '_2xg6Ul')\n",
    "    Rev_Sum6 = []\n",
    "    for i in rev_sum6:\n",
    "        Rev_Sum6.append(i.text)\n",
    "\n",
    "    rev6 = soup6.find_all('div', class_ = 'qwjRop')\n",
    "    Review6 = []\n",
    "    for i in rev6:\n",
    "        Review6.append(i.text)\n",
    "        \n",
    "        \n",
    "\n",
    "    rating7 = soup7.find_all('div', class_ = 'hGSR34 E_uFuv')\n",
    "    Ratings7 = []\n",
    "    for i in rating7:\n",
    "        Ratings7.append(i.text)\n",
    "\n",
    "    rev_sum7 = soup7.find_all('p', class_ = '_2xg6Ul')\n",
    "    Rev_Sum7 = []\n",
    "    for i in rev_sum7:\n",
    "        Rev_Sum7.append(i.text)\n",
    "\n",
    "    rev7 = soup7.find_all('div', class_ = 'qwjRop')\n",
    "    Review7 = []\n",
    "    for i in rev7:\n",
    "        Review7.append(i.text)\n",
    "        \n",
    "        \n",
    "    rating8 = soup8.find_all('div', class_ = 'hGSR34 E_uFuv')\n",
    "    Ratings8 = []\n",
    "    for i in rating8:\n",
    "        Ratings8.append(i.text)\n",
    "\n",
    "    rev_sum8 = soup8.find_all('p', class_ = '_2xg6Ul')\n",
    "    Rev_Sum8 = []\n",
    "    for i in rev_sum8:\n",
    "        Rev_Sum8.append(i.text)\n",
    "\n",
    "    rev8 = soup8.find_all('div', class_ = 'qwjRop')\n",
    "    Review8 = []\n",
    "    for i in rev8:\n",
    "        Review8.append(i.text)\n",
    "        \n",
    "        \n",
    "    rating9 = soup9.find_all('div', class_ = 'hGSR34 E_uFuv')\n",
    "    Ratings9 = []\n",
    "    for i in rating9:\n",
    "        Ratings9.append(i.text)\n",
    "\n",
    "    rev_sum9 = soup9.find_all('p', class_ = '_2xg6Ul')\n",
    "    Rev_Sum9 = []\n",
    "    for i in rev_sum9:\n",
    "        Rev_Sum9.append(i.text)\n",
    "\n",
    "    rev9 = soup9.find_all('div', class_ = 'qwjRop')\n",
    "    Review9 = []\n",
    "    for i in rev9:\n",
    "        Review9.append(i.text)\n",
    "        \n",
    "        \n",
    "    rating10 = soup10.find_all('div', class_ = 'hGSR34 E_uFuv')\n",
    "    Ratings10 = []\n",
    "    for i in rating10:\n",
    "        Ratings10.append(i.text)\n",
    "\n",
    "    rev_sum10 = soup10.find_all('p', class_ = '_2xg6Ul')\n",
    "    Rev_Sum10 = []\n",
    "    for i in rev_sum10:\n",
    "        Rev_Sum10.append(i.text)\n",
    "\n",
    "    rev10 = soup10.find_all('div', class_ = 'qwjRop')\n",
    "    Review10 = []\n",
    "    for i in rev10:\n",
    "        Review10.append(i.text)\n",
    "        \n",
    "        \n",
    "    RATING = Ratings + Ratings2 + Ratings3 + Ratings4 + Ratings5 + Ratings6 + Ratings7 + Ratings8 + Ratings9 + Ratings10\n",
    "    SUMMARY_REVIEW = Rev_Sum + Rev_Sum2 + Rev_Sum3 + Rev_Sum4 + Rev_Sum5 + Rev_Sum6 + Rev_Sum7 + Rev_Sum8 + Rev_Sum9 + Rev_Sum10\n",
    "    FULL_REVIEW = Review + Review2 + Review3 + Review4 + Review5 + Review6 + Review7 + Review8 + Review9 + Review10\n",
    "    \n",
    "    \n",
    "    # Creating a dataframe to store the scrapped information\n",
    "    Reviews_Data = pd.DataFrame({'RATING' : RATING, 'SUMMARY_REVIEW' : SUMMARY_REVIEW, 'FULL_REVIEW' : FULL_REVIEW})\n",
    "\n",
    "    # Returning the dataframe\n",
    "    return Reviews_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RATING</th>\n",
       "      <th>SUMMARY_REVIEW</th>\n",
       "      <th>FULL_REVIEW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Amazing Powerful and Durable Gadget.I’m am ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>It’s a must buy who is looking for an upgrade ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>iphone 11 is a very good phone to buy only if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Value for money❤️❤️Its awesome mobile phone in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Terrific!!! Lucky to get this phone in first l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>3</td>\n",
       "      <td>Nice</td>\n",
       "      <td>Iphone 11 black 64gb is really a cool phone 1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3</td>\n",
       "      <td>Nice</td>\n",
       "      <td>Although it’s an iPhone, it doesn’t give anyth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Just wow!</td>\n",
       "      <td>Apple i Phone is the best phone available in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>use outside gives a outstanding experience ......</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   RATING      SUMMARY_REVIEW  \\\n",
       "0       5    Perfect product!   \n",
       "1       5       Great product   \n",
       "2       5    Perfect product!   \n",
       "3       5  Highly recommended   \n",
       "4       5    Perfect product!   \n",
       "..    ...                 ...   \n",
       "95      5       Great product   \n",
       "96      3                Nice   \n",
       "97      3                Nice   \n",
       "98      5           Just wow!   \n",
       "99      5           Brilliant   \n",
       "\n",
       "                                          FULL_REVIEW  \n",
       "0   Amazing phone with great cameras and better ba...  \n",
       "1   Amazing Powerful and Durable Gadget.I’m am ver...  \n",
       "2   It’s a must buy who is looking for an upgrade ...  \n",
       "3   iphone 11 is a very good phone to buy only if ...  \n",
       "4   Value for money❤️❤️Its awesome mobile phone in...  \n",
       "..                                                ...  \n",
       "95  Terrific!!! Lucky to get this phone in first l...  \n",
       "96  Iphone 11 black 64gb is really a cool phone 1....  \n",
       "97  Although it’s an iPhone, it doesn’t give anyth...  \n",
       "98  Apple i Phone is the best phone available in t...  \n",
       "99  use outside gives a outstanding experience ......  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q7()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "***Q8: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field.***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Q8():\n",
    "    \n",
    "    # Importing essential libraries\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import requests\n",
    "    import urllib.request\n",
    "    from urllib.request import urlopen\n",
    "    \n",
    "    # Configure the webdriver\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(\"https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    # Scrapping the brand name - page 1\n",
    "    brand = soup.find_all('div', class_ = '_2B_pmu')\n",
    "    Brand = []\n",
    "    for i in brand:\n",
    "        Brand.append(i.text)\n",
    "        \n",
    "    # Scrapping the desc - page 1\n",
    "    desc = soup.find_all('a', class_ = '_2mylT6')\n",
    "    Product_Description = []\n",
    "    for i in desc:\n",
    "        Product_Description.append(i.text)\n",
    "        \n",
    "    # Scrapping the price - page 1\n",
    "    price = soup.find_all('div', class_ = '_1vC4OE')[0:40]\n",
    "    Price = []\n",
    "    for i in price:\n",
    "        Price.append(i.text)\n",
    "\n",
    "    # Scrapping the discounts - page 1\n",
    "    discount = soup.find_all('div', class_ = 'VGWI6T')[0:40]\n",
    "    Discount = []\n",
    "    for i in discount:\n",
    "        Discount.append(i.text)\n",
    "      \n",
    "    # Getting drivers for page 2\n",
    "    driver.get(\"https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page=2\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup2 = BeautifulSoup(content)\n",
    "    \n",
    "    # Scrapping the brand - page 2\n",
    "    brand2 = soup2.find_all('div', class_ = '_2B_pmu')\n",
    "    Brand2 = []\n",
    "    for i in brand2:\n",
    "        Brand2.append(i.text)\n",
    "\n",
    "    # Scrapping the sescription - page 2\n",
    "    desc2 = soup2.find_all('a', class_ = '_2mylT6')\n",
    "    Product_Description2 = []\n",
    "    for i in desc2:\n",
    "        Product_Description2.append(i.text)\n",
    "\n",
    "    # Scrapping the price - page 2\n",
    "    price2 = soup2.find_all('div', class_ = '_1vC4OE')[0:40]\n",
    "    Price2 = []\n",
    "    for i in price2:\n",
    "        Price2.append(i.text)\n",
    "\n",
    "    # Scrappig the discounts - page 2\n",
    "    discount2 = soup2.find_all('div', class_ = 'VGWI6T')[0:40]\n",
    "    Discount2 = []\n",
    "    for i in discount2:\n",
    "        Discount2.append(i.text)\n",
    "        \n",
    "    # Getting the drivers for page 3 \n",
    "    driver.get(\"https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page=3\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup3 = BeautifulSoup(content)\n",
    "    \n",
    "    # Scrapping the brand for page 3\n",
    "    brand3 = soup3.find_all('div', class_ = '_2B_pmu')\n",
    "    Brand3 = []\n",
    "    for i in brand3:\n",
    "        Brand3.append(i.text)\n",
    "\n",
    "    # Scrapping description for page 3\n",
    "    desc3 = soup3.find_all('a', class_ = '_2mylT6')\n",
    "    Product_Description3 = []\n",
    "    for i in desc3:\n",
    "        Product_Description3.append(i.text)\n",
    "\n",
    "    # Scrapping price for page 3\n",
    "    price3 = soup3.find_all('div', class_ = '_1vC4OE')[0:40]\n",
    "    Price3 = []\n",
    "    for i in price3:\n",
    "        Price3.append(i.text)\n",
    "\n",
    "    # Scrapping discounts for page 3\n",
    "    discount3 = soup3.find_all('div', class_ = 'VGWI6T')[0:40]\n",
    "    Discount3 = []\n",
    "    for i in discount3:\n",
    "        Discount3.append(i.text)\n",
    "        \n",
    "    # Appending \n",
    "    BRAND = Brand + Brand2 + Brand3\n",
    "    PRODUCT_DESCRIPTION = Product_Description + Product_Description2 + Product_Description3\n",
    "    PRICE = Price + Price2 + Price3\n",
    "    DISCOUNT = Discount + Discount2 + Discount3\n",
    "    \n",
    "    \n",
    "    # Creating a dataframe to store the scrapped information\n",
    "    Sneakers_Data = pd.DataFrame({'BRAND' : BRAND[0:100], 'PRODUCT_DESCRIPTION' : PRODUCT_DESCRIPTION[0:100], 'PRICE' : PRICE[0:100], 'DISCOUNT' : DISCOUNT[0:100]})\n",
    "\n",
    "    # Returning the dataframe\n",
    "    return Sneakers_Data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BRAND</th>\n",
       "      <th>PRODUCT_DESCRIPTION</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>DISCOUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Combo Pack of 4 Casual Sneakers With Sneakers ...</td>\n",
       "      <td>₹461</td>\n",
       "      <td>76% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>171 Smart Tan Lace-Ups Casuals for Men Sneaker...</td>\n",
       "      <td>₹236</td>\n",
       "      <td>52% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Chevit Latest Fashion Combo Pack of 2 Pairs Ca...</td>\n",
       "      <td>₹525</td>\n",
       "      <td>64% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hotstyle</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹230</td>\n",
       "      <td>53% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>170 Smart Grey Lace-Ups Casuals for Men Sneake...</td>\n",
       "      <td>₹236</td>\n",
       "      <td>52% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Claptrap</td>\n",
       "      <td>Mesh Walking Casual Sneakers Shoes for Men And...</td>\n",
       "      <td>₹449</td>\n",
       "      <td>10% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>believe</td>\n",
       "      <td>Sneakers for men Sneakers For Men</td>\n",
       "      <td>₹399</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Asian</td>\n",
       "      <td>Skypy-31 Walking Shoes,Training Shoes,Sneakers...</td>\n",
       "      <td>₹473</td>\n",
       "      <td>21% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Birde</td>\n",
       "      <td>Combo Pack of 2 Casual Shoes Sneakers For Men</td>\n",
       "      <td>₹498</td>\n",
       "      <td>50% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Rontex</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹426</td>\n",
       "      <td>57% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       BRAND                                PRODUCT_DESCRIPTION PRICE DISCOUNT\n",
       "0     Chevit  Combo Pack of 4 Casual Sneakers With Sneakers ...  ₹461  76% off\n",
       "1     Chevit  171 Smart Tan Lace-Ups Casuals for Men Sneaker...  ₹236  52% off\n",
       "2     Chevit  Chevit Latest Fashion Combo Pack of 2 Pairs Ca...  ₹525  64% off\n",
       "3   Hotstyle                                   Sneakers For Men  ₹230  53% off\n",
       "4     Chevit  170 Smart Grey Lace-Ups Casuals for Men Sneake...  ₹236  52% off\n",
       "..       ...                                                ...   ...      ...\n",
       "95  Claptrap  Mesh Walking Casual Sneakers Shoes for Men And...  ₹449  10% off\n",
       "96   believe                  Sneakers for men Sneakers For Men  ₹399  60% off\n",
       "97     Asian  Skypy-31 Walking Shoes,Training Shoes,Sneakers...  ₹473  21% off\n",
       "98     Birde      Combo Pack of 2 Casual Shoes Sneakers For Men  ₹498  50% off\n",
       "99    Rontex                                   Sneakers For Men  ₹426  57% off\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q8()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***Q9: Go to the link - https://www.myntra.com/shoes Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black” And then scrape First 100 shoes data you get***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q9():\n",
    "    \n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import requests\n",
    "    import urllib.request\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "    # Configure the webdriver\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(\"https://www.myntra.com/shoes?f=Color%3ABlack_36454f&rf=Price%3A6649.0_13099.0_6649.0%20TO%2013099.0\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    \n",
    "    brand = soup.find_all('h3', class_ = 'product-brand')\n",
    "    Brand = []\n",
    "    for i in brand:\n",
    "        Brand.append(i.text)\n",
    "    len(Brand)\n",
    "    \n",
    "    \n",
    "    desc = soup.find_all('h4', class_ = 'product-product')\n",
    "    Description = []\n",
    "    for i in desc:\n",
    "        Description.append(i.text)\n",
    "    len(Description)\n",
    "    \n",
    "    \n",
    "    price = soup.find_all('div', class_ = 'product-price')\n",
    "    Price = []\n",
    "    for i in price:\n",
    "        Price.append(i.span.text)\n",
    "    len(Price)\n",
    "    \n",
    "    \n",
    "    driver.get(\"https://www.myntra.com/shoes?f=Color%3ABlack_36454f&p=2&rf=Price%3A6649.0_13099.0_6649.0%20TO%2013099.0\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup2 = BeautifulSoup(content)\n",
    "    \n",
    "    \n",
    "    brand2 = soup2.find_all('h3', class_ = 'product-brand')\n",
    "    Brand2 = []\n",
    "    for i in brand2:\n",
    "        Brand2.append(i.text)\n",
    "    len(Brand2)\n",
    "    \n",
    "    \n",
    "    desc2 = soup2.find_all('h4', class_ = 'product-product')\n",
    "    Description2 = []\n",
    "    for i in desc2:\n",
    "        Description2.append(i.text)\n",
    "    len(Description2)\n",
    "    \n",
    "    \n",
    "    price2 = soup2.find_all('div', class_ = 'product-price')\n",
    "    Price2 = []\n",
    "    for i in price2:\n",
    "        Price2.append(i.span.text)\n",
    "    len(Price2)\n",
    "    \n",
    "    \n",
    "    BRAND = Brand + Brand2\n",
    "    DESCRIPTION = Description + Description2\n",
    "    PRICE = Price + Price2\n",
    "    \n",
    "    \n",
    "    # Creating a dataframe to store the scrapped information\n",
    "    Shoes_Data = pd.DataFrame({'BRAND' : BRAND[0:100], 'DESCRIPTION' : DESCRIPTION[0:100], 'PRICE' : PRICE[0:100]})\n",
    "    \n",
    "\n",
    "    # Returning the dataframe\n",
    "    return Shoes_Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BRAND</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADIDAS</td>\n",
       "      <td>Men FluidFlow Running Shoes</td>\n",
       "      <td>Rs. 7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADIDAS Originals</td>\n",
       "      <td>Men NMD R1 Sneakers</td>\n",
       "      <td>Rs. 10299Rs. 12999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADIDAS Originals</td>\n",
       "      <td>Men Nite Jogger Shoes</td>\n",
       "      <td>Rs. 11199Rs. 13999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADIDAS Originals</td>\n",
       "      <td>Stan Smith Leather Sneakers</td>\n",
       "      <td>Rs. 7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADIDAS Originals</td>\n",
       "      <td>Men Solid AR Training Shoes</td>\n",
       "      <td>Rs. 6399Rs. 7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Cole Haan</td>\n",
       "      <td>Men Traveller Leather Penny Loafers</td>\n",
       "      <td>Rs. 9999Rs. 19999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Cole Haan</td>\n",
       "      <td>Men Leather HARRISON GRAND 2.0 OXFORD</td>\n",
       "      <td>Rs. 9999Rs. 19999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Heel &amp; Buckle London</td>\n",
       "      <td>Men Velvet Loafers</td>\n",
       "      <td>Rs. 7693Rs. 10990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>DIESEL</td>\n",
       "      <td>Men Mid-Top Sneakers</td>\n",
       "      <td>Rs. 7800Rs. 13000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Men Leather Driving Shoes</td>\n",
       "      <td>Rs. 7999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   BRAND                            DESCRIPTION  \\\n",
       "0                 ADIDAS            Men FluidFlow Running Shoes   \n",
       "1       ADIDAS Originals                    Men NMD R1 Sneakers   \n",
       "2       ADIDAS Originals                  Men Nite Jogger Shoes   \n",
       "3       ADIDAS Originals            Stan Smith Leather Sneakers   \n",
       "4       ADIDAS Originals            Men Solid AR Training Shoes   \n",
       "..                   ...                                    ...   \n",
       "95             Cole Haan    Men Traveller Leather Penny Loafers   \n",
       "96             Cole Haan  Men Leather HARRISON GRAND 2.0 OXFORD   \n",
       "97  Heel & Buckle London                     Men Velvet Loafers   \n",
       "98                DIESEL                   Men Mid-Top Sneakers   \n",
       "99                  Geox              Men Leather Driving Shoes   \n",
       "\n",
       "                 PRICE  \n",
       "0             Rs. 7999  \n",
       "1   Rs. 10299Rs. 12999  \n",
       "2   Rs. 11199Rs. 13999  \n",
       "3             Rs. 7999  \n",
       "4     Rs. 6399Rs. 7999  \n",
       "..                 ...  \n",
       "95   Rs. 9999Rs. 19999  \n",
       "96   Rs. 9999Rs. 19999  \n",
       "97   Rs. 7693Rs. 10990  \n",
       "98   Rs. 7800Rs. 13000  \n",
       "99            Rs. 7999  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q9()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "***Q10: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then set CPU Type filter to “Intel Core i7” and “Intel Core i9”.After setting the filters scrape first 10 laptops data***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Q10():\n",
    "    \n",
    "    # Importing essential libraries\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import requests\n",
    "    import urllib.request\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "    # Configure the webdriver\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(\"https://www.amazon.in/s?k=Laptop&i=computers&rh=n%3A1375424031%2Cp_n_feature_thirteen_browse-bin%3A12598163031%7C16757432031&dc&qid=1604585877&rnid=12598141031&ref=sr_nr_p_n_feature_thirteen_browse-bin_17\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)  \n",
    "    \n",
    "    # Scraping title\n",
    "    title = soup.find_all('span', class_ = 'a-size-medium a-color-base a-text-normal')[0:10]\n",
    "    Title = []\n",
    "    for i in title:\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    # Scraping Ratings\n",
    "    rating = soup.find_all('a', class_ = 'a-popover-trigger a-declarative')[0:10]\n",
    "    Ratings = []\n",
    "    for i in rating:\n",
    "        Ratings.append(i.text)\n",
    "\n",
    "    \n",
    "    # Scrapping the Price\n",
    "    price = soup.find_all('span', class_ = 'a-price-whole')[0:10]\n",
    "    Price = []\n",
    "    for i in price:\n",
    "        Price.append(i.text)\n",
    "    len(Price)\n",
    "    \n",
    "    \n",
    "   \n",
    "    # Creating a dataframe to store the scrapped information\n",
    "    Laptop_Data = pd.DataFrame({'Title' : Title[0:10], 'Ratings' : Ratings[0:10], 'Price' : Price[0:10]})\n",
    "\n",
    "    # Returning the dataframe\n",
    "    return Laptop_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DELL XPS 9500 15.6-inch FHD Laptop (10th Gen C...</td>\n",
       "      <td>3.4 out of 5 stars</td>\n",
       "      <td>1,85,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dell XPS 9370 13.3-inch FHD Display Thin &amp; Lig...</td>\n",
       "      <td>2.7 out of 5 stars</td>\n",
       "      <td>1,18,994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HP Pavilion x360 Core i7 8th Gen 14-inch Touch...</td>\n",
       "      <td>4.1 out of 5 stars</td>\n",
       "      <td>81,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HP Pavilion x360 Touchscreen 2-in-1 FHD 14-inc...</td>\n",
       "      <td>3.4 out of 5 stars</td>\n",
       "      <td>76,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASUS TUF Gaming F15 Laptop 15.6\" FHD Intel Cor...</td>\n",
       "      <td>4.2 out of 5 stars</td>\n",
       "      <td>73,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lenovo Legion 5i 10th Gen Intel Core i7 15.6 i...</td>\n",
       "      <td>4.3 out of 5 stars</td>\n",
       "      <td>78,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ASUS TUF Gaming F15 Laptop 15.6\" FHD 144Hz Int...</td>\n",
       "      <td>4.0 out of 5 stars</td>\n",
       "      <td>83,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lenovo Legion 5Pi 10th Gen Intel Core i7 15.6\"...</td>\n",
       "      <td>3.8 out of 5 stars</td>\n",
       "      <td>1,29,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dell G3 3500 Gaming Laptop 15.6-inch FHD 120 H...</td>\n",
       "      <td>3.9 out of 5 stars</td>\n",
       "      <td>88,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Acer Nitro 5 Intel Core i7 10750H 15.6\" FHD IP...</td>\n",
       "      <td>3.4 out of 5 stars</td>\n",
       "      <td>79,990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title             Ratings  \\\n",
       "0  DELL XPS 9500 15.6-inch FHD Laptop (10th Gen C...  3.4 out of 5 stars   \n",
       "1  Dell XPS 9370 13.3-inch FHD Display Thin & Lig...  2.7 out of 5 stars   \n",
       "2  HP Pavilion x360 Core i7 8th Gen 14-inch Touch...  4.1 out of 5 stars   \n",
       "3  HP Pavilion x360 Touchscreen 2-in-1 FHD 14-inc...  3.4 out of 5 stars   \n",
       "4  ASUS TUF Gaming F15 Laptop 15.6\" FHD Intel Cor...  4.2 out of 5 stars   \n",
       "5  Lenovo Legion 5i 10th Gen Intel Core i7 15.6 i...  4.3 out of 5 stars   \n",
       "6  ASUS TUF Gaming F15 Laptop 15.6\" FHD 144Hz Int...  4.0 out of 5 stars   \n",
       "7  Lenovo Legion 5Pi 10th Gen Intel Core i7 15.6\"...  3.8 out of 5 stars   \n",
       "8  Dell G3 3500 Gaming Laptop 15.6-inch FHD 120 H...  3.9 out of 5 stars   \n",
       "9  Acer Nitro 5 Intel Core i7 10750H 15.6\" FHD IP...  3.4 out of 5 stars   \n",
       "\n",
       "      Price  \n",
       "0  1,85,990  \n",
       "1  1,18,994  \n",
       "2    81,990  \n",
       "3    76,990  \n",
       "4    73,990  \n",
       "5    78,990  \n",
       "6    83,990  \n",
       "7  1,29,990  \n",
       "8    88,990  \n",
       "9    79,990  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
